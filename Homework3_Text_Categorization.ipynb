{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 67-212 Assignment 3: Building a text categorization software\n",
    "**Due: Wednesday October 21st, 2020 at 11:59PM**\n",
    "\n",
    "In this assignment you will:\n",
    "\t\n",
    "1.\tImplement a simple preprocessing pipeline for English\n",
    "2.\tImplement a Naïve Bayes classifier\n",
    "3.\tExperiment and evaluate the classifier on text categorization. \n",
    "\n",
    "### Overview: \n",
    "You will be creating a Naïve Bayes (NB) classifier, which reads in a document and assigns a topic label to the document. You are going to train and test your classifier on discussion-board messages which were collected from online forums. \n",
    "The document topic is one of the following:\n",
    "\n",
    "    (1) Cars/Sports \n",
    "    (2) Politics \n",
    "    (3) Science\n",
    "    (4) Computers\n",
    "\n",
    "After reviewing the theoretical concepts (**task-0**), you will first annotate a small collection of documents to get familiar with the data and the challenges of text categorization (**task-1**). You will then implement a preprocessing module to tokenize the text (**task-2**). You continue by implementing the training and test functions of the NB algorithm (**task-3**). You also implement a simple function to evaluate your classifier (**task-4**). Finally, you train, test and evaluate your classifier with the provided dataset, with experimental set up. \n",
    "\n",
    "**Important Note:** All your code should be documented, and any decisions should be justifed. Please avoid adding comments in cells, but rather add a markdown text cell explaining your decisions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables:\n",
    "1.\tThe `userid.annot.txt` index file for your annotations (for task-1)\n",
    "\n",
    "2.\tThis notebook containing the following functions:\n",
    "\n",
    "    a.\t`preprocess ()`, a function that takes some text as input and tokenizes it (for task-2)\n",
    "    \n",
    "    b.\t`build_naïve_bayes()`, a function defining Naïve Bayes text categorization  (for task-3)\n",
    "    \n",
    "    c.\t`evaluate()`, a function for the evaluation of your text categorization software (for task-4)\n",
    "    \n",
    "    d.\tA textual description explaining each function and how to run it (parameters)\n",
    "\n",
    "### Data\n",
    "All the data required for this assignment is available at:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Reading and exploration\n",
    "\n",
    "Review the slides for the text classification and the Naïve Bayes algorithm in the `Text Classification: The pipeline` lecture slides (covered on September 28, 2020)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Annotation (25 points)\n",
    "\n",
    "Go to the following directory: `data/annotate`\n",
    "\n",
    "You are given 20 articles and are expected to examine them, skim through their text and label them with one of the following four labels: `comp`, `politics`, `car-sport`, `science`.\n",
    "\n",
    "To record your annotations, you are given a `sample.annot.txt` file. There are a few fake annotations in that file which shows how you're expected to record your annotations. \n",
    "\n",
    "Make a copy of this file, name it `userid1_userid2.annot.txt`. Remove the fake annotations and start recording the annotations for the 20 articles. \n",
    "\n",
    "You will submit the `userid1_userid2.annot.txt` as one deliverable for this homework.\n",
    "\n",
    "Add a markdown cell in your notebook that briefly explains your annotation experience and any interesting thing that you might have observed in the real-world data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Preprocessing (5 points)\n",
    "\n",
    "As discussed in class, it is important to perform proper preprocessing on the text to exclude the unnecessary parts of the article and tokenize the text (e.g. separating the punctuations). Based on your experience of the annotated data (Task-1), write a Python function `preprocess (text)` which reads in `text`, a document (text file), preprocesses it and generates a new tokenized file. Your function is expected to address at least two kinds of clean up and tokenization issue (e.g. one can be separation of punctuations from words, or stopword removal, or lemmatization, etc.). On top of your code, document the types of preprocessing steps that you’ve implemented. \n",
    "\n",
    "Run your function on the following article: `annotate/a1.txt` and create a tokenized document named `a1.tok.txt`.\n",
    "After adding proper documentation, you will have to submit `a1.tok.txt` as a deliverable for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Implementing Naïve Bayes algorithm from scratch (40 points) \n",
    "\n",
    "Now that we have our dataset in the format that we need, we can move onto the next portion of our mission which is the algorithm we will use to make our predictions to classify a document into one of the four categories. \n",
    "\n",
    "You will have to implement your own Naïve Bayes classifier (from scratch) in a python function named: `naive_bayes()`.\n",
    "\n",
    "As discussed in class, the learning algorithm has two major components: \n",
    "\n",
    "    (a) training the classifier with the labeled articles \n",
    "    (b) Testing the classifier with an unseen article.\n",
    "    \n",
    "**Note on preprocessing the documents:** In both of the above training and testing the classifier, your NB program is expected to use the preprocessed/tokenized documents. Thus, you need to use your processing function (from task-2. It is preferable (and more efficient) that this preprocessing takes place on-the-fly (by a call to your function). However, it would be ok if you preprocess the training and test documents separately offline and let the classifier use the preprocessed files.\n",
    "\n",
    "**Training Naïve Bayes**: Your function reads in a set of training documents (files) along with their class value (stored in the index-train.txt). After preprocessing the document, will be used to train a Naïve Bayes model. The function includes two major components: (a) The prior class probability `Prob(c)` (b) The likelihood: `Prob(word |class)`. Review the Naïve Bayes lecture to remember the two probabilities that you need to compute in the training step.\n",
    "\n",
    "**Important**: Remember to use Laplace add-one smoothing in your calculation of conditional probabilities (as explained in lecture slides). In order to have a quick look up access to the probabilities, Python's dictionaries are the more efficient data structure to use. \n",
    "\n",
    "**Testing the Naïve Bayes**: After training is completed, your program goes into the testing step. Here, the program reads in a set of test documents (files), preprocesses them and uses the Naïve Bayes model to classify them. Thus, given a test document, your program should compute the class probability for each of the four topics to choose the class with the highest probability. Your program generates an output file named `userid1-userid2-nb-test.txt` in which it outputs the class prediction for the entire test set. The format of this output file is similar to the `index-train.txt` and `index-test.txt` files. \n",
    "\n",
    "**Important**: You are provided with three sets of documents: the training set, development set (the set you annotated could be used as a development set) and the test set. All your exploration and evaluations during the implementation is done on the training and the development sets. You should use the test set only at the end when you are completely done with the implementation and are ready to evaluate the system (For more details, please check Text Classification Evaluation lecture slides).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-4: Implementing an evaluation function (10 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you implement a function that reads in the gold-standard class file (e.g. `index-test.txt` and the Naïve Bayes class file (`userid1-userid2-index.txt`) and computes the performance of the classifier in terms of accuracy, precision, recall and F1 scores. For that,  the function simply compares the NB predicted class against the gold-standard class and calculates each score. \n",
    "\n",
    "Note: you can use the scikit-learn predefined functions for calculating Precision, Recall, F1 and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-5: Experiments and write up (20 points)\n",
    "\n",
    "Perform the following experiments with your NB classifier and answer the following questions. Record your answers in a markdown cell below.\n",
    "\n",
    "1.\tRun your Naive Bayes function which would train on the training set and test on the test set. What is the F1 score of your classifier? \n",
    "\n",
    "2.\tRedo the above experiment, with the original documents without applying the preprocessing (both on the training and test documents). What is the F1 score of your classifier? How do you explain the difference between this and the previous experiment? \n",
    "\n",
    "3.\tRedo the first experiment, but this time instead of using the test set, use the same training set to test the classifier. What is the F1 score of the classifier? How do you explain the difference between this experiment and the first experiment? \n",
    "\n",
    "4.\tRedo the first experiment, but this time limit the training documents only to files with names like `2*.txt` (all text files that start with character 2). This amounts to about 10% of the training documents. Test and evaluate the classifier on the full test set. What is the F1 score of the classifier? How do you explain the difference between this experiment and the first experiment? \n",
    "\n",
    "5.\tRepeat experiment 4, but this time limit the training to files like `2*.txt, 3.*.txt, 4*.txt`. Test and evaluate the classifier on the full test set. What is the F1 score of the classifier? Compare the results with experiment 1 and 4. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
